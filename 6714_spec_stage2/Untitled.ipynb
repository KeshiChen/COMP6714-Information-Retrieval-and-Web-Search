{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 3, 2])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.]],\n",
      "\n",
      "        [[ 6.,  7.],\n",
      "         [ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[ 9.,  9.],\n",
      "         [ 9.,  9.],\n",
      "         [ 9.,  9.]],\n",
      "\n",
      "        [[ 8.,  8.],\n",
      "         [ 8.,  8.],\n",
      "         [ 8.,  8.]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[-1.7508, -0.4469, -0.5798, -0.1931],\n",
      "        [-0.7578, -1.5895, -0.6888,  0.1251],\n",
      "        [ 0.1907,  1.3085, -0.9141, -0.6718],\n",
      "        [-0.3653, -1.0359, -0.5003,  1.0438]])\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]])\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 6., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "a = [[[1.,2],[3.,4],[5.,6]],[[6.,7],[8.,9],[10.,11]]]\n",
    "b = [[[9.,9.],[9.,9.],[9.,9.]],[[8.,8.],[8.,8.],[8.,8.]]]\n",
    "a = torch.Tensor(a)\n",
    "b = torch.Tensor(b)\n",
    "#print(np.concatenate((a, b), axis=1))\n",
    "a = torch.FloatTensor(a)\n",
    "b = torch.FloatTensor(b)\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(torch.cat([a,b], dim=0))\n",
    "print(torch.cat([a,b], dim=-1).size())\n",
    "c = torch.randn(4, 4)\n",
    "print(c)\n",
    "c.view([2,-1,4])\n",
    "a = torch.Tensor([[1,2,3,4],[5,6,7,8]])\n",
    "b = a.clone()\n",
    "for i, row in enumerate(b):\n",
    "    b[i][0] = 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-dc3a264c8163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([torch.tensor([3,4]), torch.tensor([1,2,3,4]), torch.tensor([1])])\n",
    "a1 = torch.Tensor([torch.tensor([3,4]), torch.tensor([1,2,3]), torch.tensor([1])])\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "b1 = torch.nn.utils.rnn.pad_sequence(a1, batch_first=True)\n",
    "x = torch.nn.utils.rnn.pad_sequence(torch.cat([a.data.tolist(),a1.data.tolist(),dim=0]), batch_first=True)\n",
    "print(a)\n",
    "print(a1)\n",
    "print(b)\n",
    "print(b1)\n",
    "perm_idx = torch.LongTensor([1,0,2])\n",
    "_, desort = torch.sort(perm_idx, descending=False)\n",
    "desort = desort.type(torch.long)\n",
    "print(perm_idx)\n",
    "print(desort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c! tensor([1, 3, 1, 2, 4, 3, 4])\n",
      "c PackedSequence(data=tensor([1, 3, 1, 2, 4, 3, 0, 4, 0]), batch_sizes=tensor([3, 2, 2, 2]))\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [3, 4, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "tensor([[3, 4, 0, 0],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 0, 0, 0]])\n",
      "tensor([[3, 4, 0],\n",
      "        [1, 2, 3],\n",
      "        [1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "sorted_b = torch.LongTensor([[1,2,3,4], [3,4,0,0],[1,0,0,0]])\n",
    "sorted_b1 = torch.LongTensor([[1,2,3,0], [3,4,0,0],[1,0,0,0]])\n",
    "c = torch.nn.utils.rnn.pack_padded_sequence(sorted_b, batch_first=True, lengths=[4,4,1])\n",
    "print(\"c!\",c.data[c.data.nonzero()].view([1,-1])[0])\n",
    "print(\"c\",c)\n",
    "#c.data = c.data[c.data.nonzero()].view([1,-1])[0].clone()\n",
    "#c1 = torch.nn.utils.rnn.pack_padded_sequence(sorted_b1, batch_first=True, lengths=[3,2,1])\n",
    "#print(c1)\n",
    "#lstm = torch.nn.LSTM(char_lstm_input_dim, self.config.char_lstm_output_dim, 1, bidirectional=True)\n",
    "d, _ = torch.nn.utils.rnn.pad_packed_sequence(c, batch_first=True)\n",
    "d1, _ = torch.nn.utils.rnn.pad_packed_sequence(c1, batch_first=True)\n",
    "print(d)\n",
    "e = d[desort]\n",
    "e1 = d1[desort]\n",
    "print(e)\n",
    "print(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ghatmasala', 'c-pakodas<EOS>', 'nicela<EOS><EOS><EOS><EOS>']\n",
      "['<EOS>v|<pad>r|M|z,', 'z*||^^J6,<EOS>', '\\rDcP\\tS<EOS><EOS><EOS><EOS>']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Since character embeddings are a bit weak in pytorch 3, this will hopefully help out\n",
    "I think these should be trainable and also, invertable!\n",
    "So you can actually recover output from the embeddings using Cos Similarity\n",
    "\"\"\"\n",
    "\n",
    "class CharacterEmbedding:\n",
    "    def __init__(self, embedding_size):\n",
    "\n",
    "        self.vocab = ['<pad>'] + list(string.printable) + ['<SOS>', '<EOS>']\n",
    "        self.embed = nn.Embedding(len(self.vocab), embedding_size)\n",
    "        self.is_cuda = False\n",
    "        self.cos = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def flatten(self, l):\n",
    "        return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "    def embedAndPack(self, seqs, batch_first=False):\n",
    "\n",
    "        vectorized_seqs = [[self.vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "\n",
    "        # get the length of each seq in your batch\n",
    "        seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "        seq_lengths = seq_lengths.cuda() if self.is_cuda else seq_lengths\n",
    "\n",
    "        # dump padding everywhere, and place seqs on the left.\n",
    "        # NOTE: you only need a tensor as big as your longest sequence\n",
    "        seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "        seq_tensor = seq_tensor.cuda() if self.is_cuda else seq_tensor\n",
    "\n",
    "        for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "            seq_tensor[idx, :seqlen] = torch.LongTensor(seq).cuda() if self.is_cuda else torch.LongTensor(seq)\n",
    "\n",
    "        # SORT YOUR TENSORS BY LENGTH!\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "        # utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "        # Otherwise, give (L,B,D) tensors\n",
    "        if not batch_first:\n",
    "            seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "\n",
    "        # embed your sequences\n",
    "        seq_tensor = self.embed(seq_tensor)\n",
    "\n",
    "        # pack them up nicely\n",
    "        return pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.is_cuda = True\n",
    "        self.embed = self.embed.cuda()\n",
    "        return self\n",
    "\n",
    "    def unpackToSequence(self, packed_output):\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        words = self.unembed(output)\n",
    "        return words\n",
    "\n",
    "    def unembed(self, embedded_sequence):\n",
    "        weights = self.embed.state_dict()['weight']\n",
    "        weights = weights.transpose(0,1).unsqueeze(0).unsqueeze(0)\n",
    "        e_sequence = embedded_sequence.unsqueeze(3).data\n",
    "        cosines = self.cos(e_sequence, weights)\n",
    "        _, indexes = torch.topk(cosines, 1, dim=2)\n",
    "\n",
    "        words = []\n",
    "        for word in indexes:\n",
    "            word_l = ''\n",
    "            for char_index in word:\n",
    "                word_l += self.vocab[char_index[0]]\n",
    "            words.append(word_l)\n",
    "        return words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    seqs = ['ghatmasala','nicela','c-pakodas']\n",
    "\n",
    "    # make model\n",
    "\n",
    "    embedding = CharacterEmbedding(embedding_size=5)\n",
    "\n",
    "    lstm = nn.LSTM(5, 5, batch_first=True)\n",
    "\n",
    "    packed_input = embedding.embedAndPack(seqs, batch_first=True)\n",
    "    words = embedding.unpackToSequence(packed_input)\n",
    "\n",
    "    print(words)\n",
    "\n",
    "    # throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "    packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "    words = embedding.unpackToSequence(packed_output)\n",
    "\n",
    "\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_sequence(model, batch_char_index_matrices, batch_word_len_lists):\n",
    "    \n",
    "    # Given an input of the size [2,7,14], we will convert it a minibatch of the shape [14,14] to \n",
    "    # represent 14 words(7 in each sentence), and 14 characters in each word.\n",
    "    ## NOTE: Please DO NOT USE for Loops to iterate over the mini-batch.\n",
    "\n",
    "    batch_size=len(batch_word_len_lists)\n",
    "    max_word_len=max([max(i) for i in batch_word_len_lists])\n",
    "    max_sent_len=max([len(i) for i in batch_word_len_lists])\n",
    "    print(max_word_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #1. Reshape\n",
    "    _14_14_resize= batch_char_index_ex_matrices.resize(bat(batch_size*max_sent_len, max_word_len)\n",
    "    _14_resize= batch_word_len_en_lists.resize(bat(batch_size*max_sent_len)\n",
    "    print(_14_14_resize.shape)\n",
    "\n",
    " )\n",
    "\n",
    "    \n",
    "    # 2. Get corresponding char_Embeddings, we will have a Final Tensor of the shape [14, 14, 50]\n",
    "    input_char_embeds =  = model.char_emb_embeds(_14_14_resize)\n",
    "    input_embeds=input_char_embeds\n",
    "    print(input_ut_embeds.shape)\n",
    "  )\n",
    "    \n",
    "    \n",
    "    #  # 3.Sort the the mini-batch wrt word-lengths, to form a pack_padded sequence.\n",
    "    perm_idx, sorted_batch_word_len_list =  = model.sort_inp_input(_14_resize)\n",
    "    sorted_input_embeds = input_embeds[perm_idx]\n",
    "    _, desorted_indices =  = torch.sort(per(perm_idx, descending=False)\n",
    "    \n",
    "    output_sequence = pack_padded_sequence(sorted_input_embeds, lengths=sorted_batch_word_len_en_list.data.tolist(), (), batch_first=True)\n",
    "\n",
    "    \n",
    "    #  # 4.Feed the the pack_padded sequence to the char_LSTM layer.\n",
    "    output_sequence, state =  = model.char_lst_lstm(output_sequence)\n",
    "    \n",
    "    \n",
    "    # 5. Get hidden state of the shape [2,14,50].\n",
    "    output_sequence= pad_packed_sequence(output_sequence, batch_first=True)\n",
    "    hidden_state=state[0]   #hidden, cell= state\n",
    "    \n",
    "    # 6. Recover the hidden_states corresponding to the sorted index.\n",
    "    result= hidden_state[:,desorted_indices]\n",
    "\n",
    "\n",
    "    # 7. Re-shape it to get a Tensor the shape [2,7,100].\n",
    "    allcat=[]\n",
    "    \n",
    "    for batch in range(0, batch_size):\n",
    "        batch_concat=[]\n",
    "        begin=batch*max_sent_len\n",
    "        end= max_sent_len*(batch+1)\n",
    "        print(begin, end)\n",
    "        for word_len in range(begin, end):\n",
    "            direction_cat= t= torch.cat((re((result[0][word_len], result[1][word_len]),0)\n",
    "            batch_ch_concat.append(dir(direction_on_cat.detach().n().numpy())\n",
    "           allcat.append(bat(batch_concat)\n",
    "\n",
    "    allcat =  = torch.tensor(all(allcat)\n",
    "        \n",
    "    return allcat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
